{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense,GlobalAveragePooling2D\n",
    "from keras.applications import MobileNet\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras_applications\\mobilenet.py:207: UserWarning: MobileNet shape is undefined. Weights for input shape (224, 224) will be loaded.\n",
      "  warnings.warn('MobileNet shape is undefined.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf_no_top.h5\n",
      "17227776/17225924 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "base_model=MobileNet(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
    "\n",
    "x=base_model.output\n",
    "x=GlobalAveragePooling2D()(x)\n",
    "x=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
    "x=Dense(1024,activation='relu')(x) #dense layer 2\n",
    "x=Dense(512,activation='relu')(x) #dense layer 3\n",
    "preds=Dense(95,activation='softmax')(x) #final layer with softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(inputs=base_model.input,outputs=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1\n",
      "1 conv1_pad\n",
      "2 conv1\n",
      "3 conv1_bn\n",
      "4 conv1_relu\n",
      "5 conv_dw_1\n",
      "6 conv_dw_1_bn\n",
      "7 conv_dw_1_relu\n",
      "8 conv_pw_1\n",
      "9 conv_pw_1_bn\n",
      "10 conv_pw_1_relu\n",
      "11 conv_pad_2\n",
      "12 conv_dw_2\n",
      "13 conv_dw_2_bn\n",
      "14 conv_dw_2_relu\n",
      "15 conv_pw_2\n",
      "16 conv_pw_2_bn\n",
      "17 conv_pw_2_relu\n",
      "18 conv_dw_3\n",
      "19 conv_dw_3_bn\n",
      "20 conv_dw_3_relu\n",
      "21 conv_pw_3\n",
      "22 conv_pw_3_bn\n",
      "23 conv_pw_3_relu\n",
      "24 conv_pad_4\n",
      "25 conv_dw_4\n",
      "26 conv_dw_4_bn\n",
      "27 conv_dw_4_relu\n",
      "28 conv_pw_4\n",
      "29 conv_pw_4_bn\n",
      "30 conv_pw_4_relu\n",
      "31 conv_dw_5\n",
      "32 conv_dw_5_bn\n",
      "33 conv_dw_5_relu\n",
      "34 conv_pw_5\n",
      "35 conv_pw_5_bn\n",
      "36 conv_pw_5_relu\n",
      "37 conv_pad_6\n",
      "38 conv_dw_6\n",
      "39 conv_dw_6_bn\n",
      "40 conv_dw_6_relu\n",
      "41 conv_pw_6\n",
      "42 conv_pw_6_bn\n",
      "43 conv_pw_6_relu\n",
      "44 conv_dw_7\n",
      "45 conv_dw_7_bn\n",
      "46 conv_dw_7_relu\n",
      "47 conv_pw_7\n",
      "48 conv_pw_7_bn\n",
      "49 conv_pw_7_relu\n",
      "50 conv_dw_8\n",
      "51 conv_dw_8_bn\n",
      "52 conv_dw_8_relu\n",
      "53 conv_pw_8\n",
      "54 conv_pw_8_bn\n",
      "55 conv_pw_8_relu\n",
      "56 conv_dw_9\n",
      "57 conv_dw_9_bn\n",
      "58 conv_dw_9_relu\n",
      "59 conv_pw_9\n",
      "60 conv_pw_9_bn\n",
      "61 conv_pw_9_relu\n",
      "62 conv_dw_10\n",
      "63 conv_dw_10_bn\n",
      "64 conv_dw_10_relu\n",
      "65 conv_pw_10\n",
      "66 conv_pw_10_bn\n",
      "67 conv_pw_10_relu\n",
      "68 conv_dw_11\n",
      "69 conv_dw_11_bn\n",
      "70 conv_dw_11_relu\n",
      "71 conv_pw_11\n",
      "72 conv_pw_11_bn\n",
      "73 conv_pw_11_relu\n",
      "74 conv_pad_12\n",
      "75 conv_dw_12\n",
      "76 conv_dw_12_bn\n",
      "77 conv_dw_12_relu\n",
      "78 conv_pw_12\n",
      "79 conv_pw_12_bn\n",
      "80 conv_pw_12_relu\n",
      "81 conv_dw_13\n",
      "82 conv_dw_13_bn\n",
      "83 conv_dw_13_relu\n",
      "84 conv_pw_13\n",
      "85 conv_pw_13_bn\n",
      "86 conv_pw_13_relu\n",
      "87 global_average_pooling2d_1\n",
      "88 dense_1\n",
      "89 dense_2\n",
      "90 dense_3\n",
      "91 dense_4\n"
     ]
    }
   ],
   "source": [
    "for i,layer in enumerate(model.layers):\n",
    "    print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable=False\n",
    "# or if we want to set the first 20 layers of the network to be non-trainable\n",
    "for layer in model.layers[:20]:\n",
    "    layer.trainable=False\n",
    "for layer in model.layers[20:]:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48905 images belonging to 95 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
    "\n",
    "train_generator=train_datagen.flow_from_directory('Fruit-Images-Dataset/Training/',\n",
    "                                                 target_size=(224,224),\n",
    "                                                 color_mode='rgb',\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1528/1528 [==============================] - 929s 608ms/step - loss: 0.3316 - acc: 0.9163\n",
      "Epoch 2/10\n",
      "1147/1528 [=====================>........] - ETA: 3:47 - loss: 0.1511 - acc: 0.9677"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "# Adam optimizer\n",
    "# loss function will be categorical cross entropy\n",
    "# evaluation metric will be accuracy\n",
    "\n",
    "step_size_train=train_generator.n//train_generator.batch_size\n",
    "model.fit_generator(generator=train_generator,\n",
    "                   steps_per_epoch=step_size_train,\n",
    "                   epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/DietGenie/ImageRecognition/food_v1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = save_model(\"/DietGenie/ImageRecognition/food_v1.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
